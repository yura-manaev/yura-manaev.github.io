/*import React, { useEffect } from 'react';
import { Helmet } from 'react-helmet';
import { motion } from 'framer-motion';
import { itemDataLearning } from './itemData';
import TelegramComments from 'react-telegram-comments';
import ChannelPreviewLearning from '../components/ChannelPreviewLearning';
import M from '../components/Markdown';
import L from '../components/Latex';
import Img from '../components/Image';

const postNumber = itemDataLearning.find(x => x.key === 'dimensionality_reduction').id;

const title = itemDataLearning.find(x => x.id === postNumber).title + ' - vladaverett.github.io';
const key = itemDataLearning.find(x => x.id === postNumber).key;
const date = itemDataLearning.find(x => x.id === postNumber).date;
var banner = require("./img/learning/" + key + ".jpg");

const prevImgStyle = {
	"width": "55%",
	"clip-path": "inset(25% 0px 30% 0px round 15px)",
	"transform": "scale(1.8)",
	"filter": "brightness(70%)"
}

const prevStyle = {
	"position": "relative",
}

const prevTextStyle = {
	"position": "absolute",
	"top": "50%",
	"left": "50%", "width": "100%",
	"transform": "translate(-50%, -70%)",
	"margin": "0",
	"padding": "0",
	"color": "#f2f2f2",
	"font-size": "55px",
	"font-family": "'Quicksand', sand-serif"
}

const prevDateStyle = {
	"position": "absolute",
	"top": "60%",
	"left": "50%", "width": "100%",
	"transform": "translate(-50%, -70%)",
	"margin": "0",
	"padding": "0",
	"color": "#f2f2f2",
	"font-size": "20px",
	"font-family": "'Quicksand', sand-serif"
}

export default function Post() {  	

	useEffect(() => {
  		window.scrollTo(0, 0)
	}, [])

	return (
		<motion.div 
			initial={{opacity: 0 }}
			animate={{opacity: 1 }}
			exit={{opacity: 0 }}
			transition={{ duration: 0.15 }}>
			<Helmet><title>{ title }</title></Helmet>
			<center><div class='noselect' style={prevStyle}>
				<img style={prevImgStyle} src={banner} alt="banner" />
				<b><div style={prevTextStyle}>{itemDataLearning.find(x => x.id === postNumber).title}</div></b>
				<div style={prevDateStyle}>{date}</div>
				<br/>
			</div></center><div class="postBody">
				
					
<M text='Principal component analysis (PCA) is a widely used dimensionality reduction technique in machine learning. It is an unsupervised learning method that aims to identify the most important features or components of a high-dimensional dataset by projecting it onto a lower-dimensional space.'/>
<L text='$\textbf{PCA}$'/> <M text='can be represented mathematically as follows: Given a dataset X consisting of n data points and p features, PCA finds a new set of p orthogonal axes, also known as principal components (PCs), that maximize the variance of the data projected onto them. The first PC captures the direction of greatest variance in the data, the second PC captures the direction of second greatest variance, and so on.'/>

<M text='The procedure of PCA can be summarized as follows: '/>
<ol>
    <li>Compute the mean vector of the dataset X: <L text='$\boldsymbol{\mu} = \frac{1}{n}\sum_{i=1}^{n}\boldsymbol{x_i}$'/></li>
    <li>Compute the covariance matrix of X: <L text='$\boldsymbol{\Sigma} = \frac{1}{n-1}\sum_{i=1}^{n}(\boldsymbol{x_i}-\boldsymbol{\mu})(\boldsymbol{x_i}-\boldsymbol{\mu})^T$'/></li>
    <li>Compute the eigenvectors and eigenvalues of the covariance matrix: <L text='$\boldsymbol{\Sigma} \boldsymbol{v} = \lambda \boldsymbol{v}$'/>, where <L text='$\boldsymbol{v}$'/> are the eigenvectors and <L text='$\lambda$'/> are the eigenvalues.</li>
    <li>Sort the eigenvectors in descending order based on their corresponding eigenvalues.</li>
    <li>Select the top k eigenvectors to form a projection matrix <L text='$\boldsymbol{W}$'/>.</li>
    <li>Transform the data into the new k-dimensional space by computing <L text='$\boldsymbol{Y} = \boldsymbol{XW}$'/>.</li>
</ol>
<M text='PCA has been used in various fields such as image processing, bioinformatics, and finance. In image processing, PCA has been used for face recognition, image compression, and denoising. In bioinformatics, PCA has been used for gene expression analysis, protein structure analysis, and DNA sequencing. In finance, PCA has been used for portfolio optimization, risk management, and asset pricing.'/>
<M text='Despite its popularity, PCA has some limitations. One of the major limitations is that it assumes that the data is linearly related, which may not be the case in some real-world datasets. Additionally, PCA may not perform well on datasets with nonlinear relationships between the features.'/>
<M text='In conclusion, PCA is a powerful technique for dimensionality reduction and feature extraction. It can be used in a wide range of applications and has been shown to be effective in many real-world scenarios. However, it is important to keep in mind its limitations and to consider alternative methods when dealing with nonlinear datasets.'/>						
							
							
			</div><div class='chatWrapper'><TelegramComments websiteKey={'2JA7Wo3q'} customColor='000000' commentsNumber={5} pageId={key} showDislikes={true} /></div>
			<ChannelPreviewLearning />
		</motion.div>
	);
};
  */
